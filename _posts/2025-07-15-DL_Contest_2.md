---
layout: post
title: "[DL, Pytorch] SW중심대학 AI 경진대회 끄적이기_2"
tags: [DL, pandas, pytorch]
math: true
date: 2025-07-15 22:00 +0900
categories:
    - Study
toc: true
---
데이콘 주소 : https://dacon.io/competitions/official/236473/overview/description
* * *
## 데이터 확인
1.train.csv   
![제목](\assets\traincsv.png){: width="40%" height="40%"}  
데이터 개수 : 약 10만개    
데이터 구성 : 제목, 글, 생성 여부    
특이사항 : 데이터 글이 길다. 매우 길다. 또한 영어가 아닌 한국어로 작성되어 있다. 이에 따른 처리 방식에 대해 고민해봐야할 것 같다.   
2.test.csv   
train.csv와 동일한 형태지만 생성 여부만 없는 전형적인 평가용 파일. 해당 파일을 분류하는게 목표.   
* * *
## 해결 방법 고민 
지난 시간에 발생했던 문제를 해결하기 위해 huggingface를 뒤졌다.   
   
https://huggingface.co/monologg/kobigbird-bert-base   
saparse-attention 기반 한국어로 학습된 모델이 있음을 확인했다.   
BERT는 512개 토큰까지 가능했지만 해당 모델은 4096개 토큰까지 사용 가능한 모델로, 현재 문제에 적합한 모델이라 생각했다.   
   
## 코드 구성
1.데이터 불러오기
2.토큰화(BertTockenizer)   
3.모델 제작   
4.학습   

## 학습 결과
![제목](\assets\bigbirdResult.png){: width="40%" height="40%"}  
^^;;;;;   
로컬에서는 130시간, 코랩 무료 환경에서는 램 전부 다 써서 런타임 종료...   
다른 방식이 필요할 듯 하다...   

## 다음 구현 방향성
일단 상대적으로 가벼운 머신러닝으로 돌리기라도 해보자;;  
XGBoost + OKT Tockenizer를 통해 상대적으로 가벼운 방식으로 학습 진행   
(XGBoost : https://www.nvidia.com/ko-kr/glossary/xgboost/)

